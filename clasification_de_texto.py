# -*- coding: utf-8 -*-
"""clasification_de_texto.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ORwGWkky0YKz84zqHyrbq1d8GkwtzHqI
"""

import tensorflow as tf
from tensorflow import keras
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import time

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("todos.csv")

df.shape

df.head()

print((df.target== 1).sum())#CORTE DE SUMINISTRO
print((df.target== 0).sum())#NO CORTE DE SUMINISTRO

#Preprocessing
import re
import string

def remove_URL(text):
  url=re.compile(r"https?://\S+|www\.\S+")
  return url.sub(r"", text)

#https://stackoverflow.com/questions/34293875/how-to-remove

def remove_punct(text):
  translator = str.maketrans("", "",string.punctuation)
  return text.translate(translator)

string.punctuation

pattern = re.compile(r"https?://(\S+|www)\.\S+")

for t in df.Text:
  matches = pattern.findall(t)
  for match in matches:
    print(t)
    print(match)
    print(pattern.sub(r"", t))
  if len(matches) > 0:
    break

df["text"] = df.Text.map(remove_URL) #map(lambdax:remove_URL(x))
df["text"] = df.Text.map(remove_punct)

#remove stopwords
#pip install nltk
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

#Stop Words:Astop word isacommonly used word(such as"the","a","an","in")thatasearch engine
#has been programmed to ignore,both when indexing entries for searching and when retrieving them
#as the result ofasearch query.
stop = set(stopwords.words("spanish"))

#https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python
def remove_stopwords(text):
  filtered_words=[word.lower() for word in text.split() if word.lower() not in stop]
  return " ".join(filtered_words)

stop

df["text"] = df.Text.map(remove_stopwords)

df.Text

from collections import Counter

#Count unique words
def counter_word(text_col):
  count = Counter()
  for text in text_col.values:
    for word in text.split():
      count[word] += 1
    return count
    
counter=counter_word(df.text)

len(counter)

counter

counter.most_common(5)

num_unique_words = len(counter)

#Split dataset into training and validation set
train_size=int(df.shape[0] * 0.8)

train_df = df[:train_size]
val_df = df[train_size:]

#split text and labels
train_sentences = train_df.text.to_numpy()
train_labels = train_df.target.to_numpy()
val_sentences = val_df.text.to_numpy()
val_labels = val_df.target.to_numpy()

train_sentences.shape, val_sentences.shape

#Tokenize
from tensorflow.keras.preprocessing.text import Tokenizer

#vectorizeatext corpus by turning each text intoasequence of integers
tokenizer=Tokenizer(num_words=num_unique_words)
tokenizer.fit_on_texts(train_sentences)#fit only to training

# cada palabra tiene su indice unico

word_index = tokenizer.word_index

#word_index

train_sequences = tokenizer.texts_to_sequences(train_sentences)
val_sequences = tokenizer.texts_to_sequences(val_sentences)

print(train_sentences[10:35])
print(train_sequences[10:35])

#Pad the sequences to have the same length
from tensorflow.keras.preprocessing.sequence import pad_sequences
#Max number of words inasequence
max_length=201
train_padded=pad_sequences(train_sequences,maxlen=max_length,padding="post",truncating="post")
val_padded=pad_sequences(val_sequences,maxlen=max_length,padding="post",truncating="post")
train_padded.shape,val_padded.shape

train_padded[10]

print(train_sentences[10])
print(train_sequences[10])
print(train_padded[10])

#Check reversing the indices

#flip(key,value)
reverse_word_index=dict([(idx,word)for(word,idx)in word_index.items()])

reverse_word_index

def decode(sequence):
  return "".join([reverse_word_index.get(idx, "?") for idx in sequence])

decoded_text = decode(train_sequences[10])

print(train_sequences[10])
print(decoded_text)

#Create LSTM model
from tensorflow.keras import layers

#Embedding:https://www.tensorflow.org/tutorials/text/word_embeddings
#Turns positive integers(indexes)into dense vectors of fixed size.(other approach could be one-hot-encoding)
#Word embeddings give usaway to use an efficient,dense representation in which similar words have
#asimilar encoding.Importantly,you do not have to specify this encoding by hand.An embedding isa
#dense vector of floating point values(the length of the vector isaparameter you specify).

model = keras.models.Sequential()
model.add(layers.Embedding(num_unique_words, 32, input_length = max_length))

#The layer will take as input an integer matrix of size(batch,input_length),
#and the largest integer(i.e. word index)in the input should be no larger than num_words(vocabulary size).
#Now model.output_shape is(None,input_length,32),where`None is the batch dimension.

model.add(layers.LSTM(64,dropout=0.1))
model.add(layers.Dense(1,activation="sigmoid"))
model.summary()

loss = keras.losses.BinaryCrossentropy(from_logits=False)
optim = keras.optimizers.Adam(lr=0.001)
metrics = ["accuracy"]

model.compile(loss=loss, optimizer = optim, metrics = metrics)

model.fit(train_padded,train_labels,epochs=20,validation_data=(val_padded,val_labels),verbose=2)

predictions = model.predict(train_padded)
predictions = [ 1 if p > 0.5 else 0 for p in predictions]

print(train_sentences[10:20])
print(train_labels[10:20])
print(predictions[10:20])

